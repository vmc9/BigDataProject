{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import lit, rand, row_number\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.memory.fraction\", 0.8) \\\n",
    "    .config(\"spark.executor.memory\", \"14g\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\" , \"800\")\\\n",
    "    .config(\"spark.sql.crossJoin.enabled\" , \"true\")\\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "mirai_devices = ['danmini_doorbell', 'ecobee_thermostat', 'philips_B120N10_baby_monitor', \n",
    "            'provision_PT_737E_security_camera', 'provision_PT_838_security_camera',\n",
    "            'simplehome_XCS_1002_WHT_security_camera', 'simplehome_XCS_1003_WHT_security_camera']\n",
    "\n",
    "benign = ['danmini_doorbell', 'ecobee_thermostat', 'ennio_doorbell', 'philips_B120N10_baby_monitor', \n",
    "            'provision_PT_737E_security_camera', 'provision_PT_838_security_camera', 'samsung_SNH_1011_N_webcam',\n",
    "            'simplehome_XCS_1002_WHT_security_camera', 'simplehome_XCS_1003_WHT_security_camera']\n",
    "\n",
    "mirai_attacks = ['ack', # automatic scan for vulnerable devices\n",
    "                 'scan', # ack flood\n",
    "                 'syn', # syn flood\n",
    "                 'udp', # udp flood\n",
    "                 'udpplain'] # optimized udp flood"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "total malign data = 3668402\ntotal benign data = 555932\n",
      "total data for prediciton modeling = 1110992\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "first = True\n",
    "mirai_data = None\n",
    "for device in mirai_devices:\n",
    "    for attack in mirai_attacks:\n",
    "        if first:\n",
    "            mirai_data = spark.read.option(\"inferSchema\",True)\\\n",
    "                .option(\"header\", True).csv(f'../data/n_balo_t/{device}/mirai_attacks/{attack}.csv')\n",
    "            first = False\n",
    "        else:\n",
    "            to_add = spark.read.option(\"inferSchema\",True)\\\n",
    "                .option(\"header\", True).csv(f'../data/n_balo_t/{device}/mirai_attacks/{attack}.csv')\n",
    "            mirai_data = mirai_data.union(to_add)\n",
    "mirai_data = mirai_data.withColumn('label', lit(0))\n",
    "malign_total = mirai_data.count()\n",
    "\n",
    "first = True\n",
    "benign_data = None\n",
    "for device in benign:\n",
    "    if first:\n",
    "        benign_data = spark.read.option(\"inferSchema\",True)\\\n",
    "            .option(\"header\", True).csv(f'../data/n_balo_t/{device}/benign_traffic.csv')\n",
    "        first = False\n",
    "    else:\n",
    "        to_add = spark.read.option(\"inferSchema\",True)\\\n",
    "            .option(\"header\", True).csv(f'../data/n_balo_t/{device}/benign_traffic.csv')\n",
    "        benign_data = benign_data.union(to_add)\n",
    "benign_data = benign_data.withColumn('label', lit(1))\n",
    "benign_total = benign_data.count()\n",
    "ratio = benign_total/malign_total\n",
    "print(f'total malign data = {malign_total}')\n",
    "print(f'total benign data = {benign_total}')\n",
    "mirai_data = mirai_data.sample(False, fraction=ratio)\n",
    "mirai_data = mirai_data.union(benign_data)\n",
    "mirai_data = mirai_data.withColumn('label', mirai_data['label'].cast(DoubleType()))\n",
    "print(f'total data for prediciton modeling = {mirai_data.count()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "cols = mirai_data.columns\n",
    "new_cols = [str(i) for i in range(len(cols))]\n",
    "mirai_data = mirai_data.toDF(*new_cols)\n",
    "mirai_data = mirai_data.withColumnRenamed('115', 'label')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "555932 555060\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "benign = mirai_data.filter(mirai_data['label']==1.0).count()\n",
    "malicious = mirai_data.filter(mirai_data['label']==0.0).count()\n",
    "print(benign, malicious)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def std(data):\n",
    "    feature_list = []\n",
    "    for col in data.columns:\n",
    "        if col == 'label':\n",
    "            continue\n",
    "        else:\n",
    "            feature_list.append(col)\n",
    "    labels = data.select('label')\n",
    "    \n",
    "    # vector assembler\n",
    "    asmblr = VectorAssembler(inputCols=feature_list, outputCol='features')\n",
    "    print(\"Assembling feature vector..\")\n",
    "    data = asmblr.transform(data).select(['features'])\n",
    "    \n",
    "    # standardize the features\n",
    "    standardizer = StandardScaler(withMean=True, withStd=True, inputCol='features', outputCol='std_features')\n",
    "    print(\"Standardizing feature vector..\")\n",
    "    standardizer_model = standardizer.fit(data)\n",
    "    data = standardizer_model.transform(data)\n",
    "    \n",
    "    # add label column\n",
    "    print(\"Adding labels..\")\n",
    "    data = data.join(labels)\n",
    "    \n",
    "    return data\n",
    "    \n",
    "def pca(data):\n",
    "    feature_list = []\n",
    "    for col in data.columns:\n",
    "        if col == 'label':\n",
    "            continue\n",
    "        else:\n",
    "            feature_list.append(col)\n",
    "    labels = data.select('label')\n",
    "    \n",
    "    # vector assembler\n",
    "    asmblr = VectorAssembler(inputCols=feature_list, outputCol='features')\n",
    "    print(\"Assembling feature vector..\")\n",
    "    data = asmblr.transform(data).select(['features'])\n",
    "    \n",
    "    # standardize the features\n",
    "    standardizer = StandardScaler(withMean=True, withStd=True, inputCol='features', outputCol='std_features')\n",
    "    print(\"Standardizing feature vector..\")\n",
    "    standardizer_model = standardizer.fit(data)\n",
    "    data = standardizer_model.transform(data)\n",
    "    \n",
    "    # use PCA estimator directly on the standardized df, to generate pca_features\n",
    "    num_principal_components = 13\n",
    "    pca = PCA(k=num_principal_components, inputCol='std_features', outputCol='pca_features')\n",
    "    print(\"PCA estimation..\")\n",
    "    pca_model = pca.fit(data)\n",
    "    data = pca_model.transform(data)\n",
    "\n",
    "    # data with principal features and label\n",
    "    data = data.drop('features').drop('std_features')\n",
    "    print(\"Replacing feature vector..\")\n",
    "    principal_features_list = feature_list[:num_principal_components]\n",
    "    data = data.select(\"pca_features\")\n",
    "    \n",
    "    # disassemble pca_features vector into individual columns\n",
    "    print(\"Assembling new dataframe..\")\n",
    "    data = data.rdd.map(lambda x:[float(y) for y in x['pca_features']]).toDF(principal_features_list)\n",
    "    \n",
    "    # add label column\n",
    "    print(\"Adding labels..\")\n",
    "    data = data.join(labels)\n",
    "    \n",
    "    # use new_data to run algorithm as before ...\n",
    "    \n",
    "    # set up feature and labels as input and output (with pca_features)\n",
    "    asmblr_2 = VectorAssembler(inputCols=principal_features_list, outputCol=\"features\")\n",
    "    print(\"Preparing new dataframe with PCA features..\")\n",
    "    data = asmblr_2.transform(data).select(['label', 'features'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "def data_prep(data):\n",
    "    feature_list = []\n",
    "    for col in data.columns:\n",
    "        if col == 'label':\n",
    "            continue\n",
    "        else:\n",
    "            feature_list.append(col)\n",
    "            \n",
    "    # set up feature and labels as input and output\n",
    "    asmblr = VectorAssembler(inputCols=feature_list, outputCol=\"features\")\n",
    "    \n",
    "    assembled_data=asmblr.transform(data)\n",
    "    \n",
    "    return assembled_data\n",
    "    \n",
    "def reg_rf(data, trees, maxD):\n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=trees, maxDepth=maxD)\n",
    "    \n",
    "    (trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "    model = rf.fit(trainingData)\n",
    "    preds = model.transform(testData)\n",
    "    return preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def printout(m):\n",
    "    print(f'Accuracy: {m.accuracy}')\n",
    "    print(f'F1: {m.fMeasure()}')\n",
    "    print(f'False Positive Rate for Benign: {m.falsePositiveRate(1.0)}')\n",
    "    print(f'True Positive Rate for Benign: {m.truePositiveRate(1.0)}')\n",
    "    print(f'False Positive Rate for Malicious: {m.falsePositiveRate(0.0)}')\n",
    "    print(f'True Positive Rate for Malicious: {m.truePositiveRate(0.0)}')\n",
    "    print(f'Precision: {m.precision()}')\n",
    "    print(f'Recall: {m.recall()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "big_sample, small_sample = mirai_data.randomSplit([0.95, 0.05])\n",
    "small_sample = small_sample.withColumn('order', rand(seed=123)).orderBy('order').drop('order')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Accuracy: 1.0\nF1: 1.0\n",
      "False Positive Rate for Benign: 0.0\nTrue Positive Rate for Benign: 1.0\nFalse Positive Rate for Malicious: 0.0\nTrue Positive Rate for Malicious: 1.0\nPrecision: 1.0\nRecall: 1.0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "prepared = data_prep(small_sample)\n",
    "predictions = reg_rf(prepared, 5, 10)\n",
    "predictions_rdd = predictions.select(['label', 'prediction']).rdd.map(tuple)\n",
    "metrics = MulticlassMetrics(predictions_rdd)\n",
    "printout(metrics)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}