{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import lit, rand, row_number\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bashlite_devices = ['danmini_doorbell', 'ecobee_thermostat', 'ennio_doorbell', 'philips_B120N10_baby_monitor',\n",
    "                    'provision_PT_737E_security_camera', 'provision_PT_838_security_camera', 'samsung_SNH_1011_N_webcam',\n",
    "                    'simplehome_XCS_1002_WHT_security_camera', 'simplehome_XCS_1003_WHT_security_camera']\n",
    "\n",
    "benign = ['danmini_doorbell', 'ecobee_thermostat', 'ennio_doorbell', 'philips_B120N10_baby_monitor',\n",
    "          'provision_PT_737E_security_camera', 'provision_PT_838_security_camera', 'samsung_SNH_1011_N_webcam',\n",
    "          'simplehome_XCS_1002_WHT_security_camera', 'simplehome_XCS_1003_WHT_security_camera']\n",
    "\n",
    "bashlite_attacks = ['scan',  # scanning a network for vulnerable devices\n",
    "                    'junk',  # sending spam data\n",
    "                    'udp',  # udp flood\n",
    "                    'tcp',  # tcp flood\n",
    "                    'combo']  # sending spam data and opening connection to specific ip/port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "first = True\n",
    "bashlite_data = None\n",
    "for device in bashlite_devices:\n",
    "    for attack in bashlite_attacks:\n",
    "        if first:\n",
    "            bashlite_data = spark.read.option(\"inferSchema\", True)\\\n",
    "                .option(\"header\", True).csv(f'../data/n_balo_t/{device}/bashlite_attacks/{attack}.csv')\n",
    "            bashlite_data = bashlite_data.withColumn(\"label\", lit(attack))\n",
    "            first = False\n",
    "        else:\n",
    "            to_add = spark.read.option(\"inferSchema\", True)\\\n",
    "                .option(\"header\", True).csv(f'../data/n_balo_t/{device}/bashlite_attacks/{attack}.csv')\n",
    "            to_add = to_add.withColumn(\"label\", lit(attack))\n",
    "            bashlite_data = bashlite_data.union(to_add)\n",
    "first = True\n",
    "benign_data = None\n",
    "for device in benign:\n",
    "    if first:\n",
    "        benign_data = spark.read.option(\"inferSchema\", True)\\\n",
    "            .option(\"header\", True).csv(f'../data/n_balo_t/{device}/benign_traffic.csv')\n",
    "        first = False\n",
    "    else:\n",
    "        to_add = spark.read.option(\"inferSchema\", True)\\\n",
    "            .option(\"header\", True).csv(f'../data/n_balo_t/{device}/benign_traffic.csv')\n",
    "        benign_data = benign_data.union(to_add)\n",
    "benign_data = benign_data.withColumn('label', lit(\"benign\"))\n",
    "\n",
    "bashlite_data = bashlite_data.union(benign_data)\n",
    "\n",
    "print(f'total data for prediciton modeling = {bashlite_data.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols = bashlite_data.columns\n",
    "new_cols = [str(i) for i in range(len(cols))]\n",
    "bashlite_data = bashlite_data.toDF(*new_cols)\n",
    "bashlite_data = bashlite_data.withColumnRenamed('115', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pie(k, num):\n",
    "    labels = [x[0] for x in k]\n",
    "\n",
    "    sizes = [x[1] for x in k]\n",
    "\n",
    "    percent = [100*(p/sum(sizes)) for p in sizes]\n",
    "\n",
    "    colors = ['tab:blue', 'tab:green', 'tab:orange',\n",
    "              'tab:red', 'tab:gray', 'tab:olive']\n",
    "\n",
    "    patches, texts = plt.pie(sizes, colors=colors, startangle=90, radius=1.2)\n",
    "    labels = ['{0} - {1:1.2f} %'.format(i, j) for i, j in zip(labels, percent)]\n",
    "\n",
    "    sort_legend = True\n",
    "    if sort_legend:\n",
    "        patches, labels, dummy = zip(*sorted(zip(patches, labels, sizes),\n",
    "                                             key=lambda x: x[2],\n",
    "                                             reverse=True))\n",
    "\n",
    "    plt.legend(patches, labels, loc='center right', bbox_to_anchor=(-0.1, 1.),\n",
    "               fontsize=8)\n",
    "\n",
    "    plt.savefig('piechart.png', bbox_inches='tight')\n",
    "\n",
    "    plt.title(f'Cluster Composition for K{num}')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def reg_kmeans(data, min_k=2, max_k=5):\n",
    "    feature_list = []\n",
    "    for col in data.columns:\n",
    "        if col == 'label':\n",
    "            continue\n",
    "        else:\n",
    "            feature_list.append(col)\n",
    "\n",
    "    # set up feature and labels as input and output\n",
    "    asmblr = VectorAssembler(inputCols=feature_list, outputCol=\"features\")\n",
    "\n",
    "    assembled_data = asmblr.transform(data)\n",
    "\n",
    "    scale = StandardScaler(inputCol='features', outputCol='standardized')\n",
    "    data_scale = scale.fit(assembled_data)\n",
    "    data_scale_output = data_scale.transform(assembled_data)\n",
    "\n",
    "    silhouette_score = []\n",
    "\n",
    "    evaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='standardized',\n",
    "                                    metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
    "\n",
    "    highest = (0, 0, None, None)\n",
    "\n",
    "    for i in range(min_k, max_k):\n",
    "        KMeans_algo = KMeans(featuresCol='standardized', k=i)\n",
    "\n",
    "        model = KMeans_algo.fit(data_scale_output)\n",
    "\n",
    "        output = model.transform(data_scale_output)\n",
    "\n",
    "        score = evaluator.evaluate(output)\n",
    "\n",
    "        if highest[0] < score:\n",
    "            highest = (score, i, output, model.clusterCenters())\n",
    "\n",
    "        silhouette_score.append(score)\n",
    "\n",
    "        print(\"Silhouette Score:\", score)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    ax.plot(range(min_k, max_k), silhouette_score)\n",
    "    ax.set_xlabel('k')\n",
    "    ax.set_ylabel('cost')\n",
    "\n",
    "    return highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "big_sample, small_sample = bashlite_data.randomSplit([0.85, 0.15])\n",
    "print(small_sample.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions = reg_kmeans(small_sample, 2, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels_clusters = predictions[2].select(['label', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(predictions[1]):\n",
    "    cluster = labels_clusters.filter(\n",
    "        labels_clusters['prediction'] == i).groupBy('label').count().collect()\n",
    "    assigned = sum([l[1] for l in cluster])\n",
    "    total = small_sample.count()\n",
    "    print(f'Total data: {total} - Data in K{i}: {assigned}')\n",
    "    print(\n",
    "        f'Cluster data assigned to K{i} as % of total data: {(assigned/total)*100}%')\n",
    "    for x in cluster:\n",
    "        print(f'{x[0]} : {x[1]}')\n",
    "    pie(cluster, i)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(predictions[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
